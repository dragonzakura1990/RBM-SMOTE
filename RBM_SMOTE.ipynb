{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#SMOTE\n",
    "def SMOTE(X, n_nbrs, n_samples):\n",
    "    synthetic_data=[]\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_nbrs+1, algorithm='auto').fit(X)\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "    for i in range(len(X)):\n",
    "        for _ in range(n_samples):\n",
    "            idx=random.randint(1, n_nbrs-1)\n",
    "            selected_nbr=indices[i][idx]\n",
    "            coef=np.random.uniform(0,1,1)[0]\n",
    "            syn_sample=coef*X[i]+(1-coef)*X[selected_nbr]\n",
    "            synthetic_data.append(syn_sample)\n",
    "    return synthetic_data\n",
    "\n",
    "\n",
    "# Simple RBM class\n",
    "def sample(probabilities, mode='continuous'):\n",
    "    ''' Sample a tensor based on the probabilities (A tensor given by get_probabilities)'''\n",
    "    if mode=='continuous':\n",
    "        return probabilities\n",
    "    elif mode=='binary':\n",
    "        return tf.floor(probabilities + tf.random.uniform(tf.shape(probabilities), 0, 1))\n",
    "\n",
    "\n",
    "\n",
    "class RBM:\n",
    "\n",
    "    def __init__(self, n_visible, n_hidden, lr, epochs, mode='continuous'):\n",
    "        ''' Initialize a model for an RBM with one layer of hidden units '''\n",
    "        self.mode = mode # bernoulli or gaussian RBM\n",
    "        self.n_hidden = n_hidden #  Number of hidden nodes\n",
    "        self.n_visible = n_visible # Number of visible nodes\n",
    "        self.lr = lr # Learning rate for the CD algorithm\n",
    "        self.epochs = epochs # Number of iterations to run the algorithm for\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        with tf.name_scope('Weights'):\n",
    "            self.W = tf.Variable(tf.random.normal([self.n_visible, self.n_hidden], mean=0., stddev=4 * np.sqrt(6. / (self.n_visible + self.n_hidden))), name=\"weights\")\n",
    "        tf.summary.histogram('weights',self.W)\n",
    "        self.vb = tf.Variable(tf.zeros([1, self.n_visible]),tf.float32, name=\"visible_bias\")\n",
    "        self.hb = tf.Variable(tf.zeros([1, self.n_hidden]),tf.float32, name=\"hidden_bias\")\n",
    "\n",
    "\n",
    "    def get_probabilities(self, layer, val):\n",
    "        ''' Return a tensor of probabilities associated with the layer specified'''\n",
    "        if layer == 'hidden':\n",
    "            with tf.name_scope(\"Hidden_Probabilities\"):\n",
    "                return tf.nn.sigmoid(tf.matmul(val, self.W) + self.hb)\n",
    "\n",
    "        elif layer == 'visible':\n",
    "            with tf.name_scope(\"Visible_Probabilities\"):\n",
    "                return tf.nn.sigmoid(tf.matmul(val, tf.transpose(self.W)) + self.vb)\n",
    "\n",
    "\n",
    "    def CD(self, v, K=1):\n",
    "        ''' K-step Contrastive Divergence using Gibbs sampling. Return parameters update. '''\n",
    "        with tf.name_scope(\"Contrastive_Divergence\"):\n",
    "            h_prob = self.get_probabilities('hidden', v)\n",
    "            h_state = sample(h_prob, mode=self.mode)\n",
    "            pos_divergence = tf.matmul(tf.transpose(v), h_prob) # Positive Divergence + h(v).v^T\n",
    "\n",
    "            fake_v_prob = self.get_probabilities('visible', h_state)\n",
    "            fake_v_state = fake_v_prob #sample(fake_v_prob)\n",
    "\n",
    "            fake_h_prob = self.get_probabilities('hidden', fake_v_state)\n",
    "            fake_h_state = sample(fake_h_prob, mode=self.mode)\n",
    "\n",
    "            for i in range(K-1): # Number of steps to run the algorithm\n",
    "\n",
    "                fake_v_prob = self.get_probabilities('visible', fake_h_state)\n",
    "                fake_v_state = fake_v_prob #sample(fake_v_prob)\n",
    "\n",
    "                fake_h_prob = self.get_probabilities('hidden', fake_v_state)\n",
    "                fake_h_state = sample(fake_h_prob, mode=self.mode)\n",
    "\n",
    "            neg_divergence = tf.matmul(tf.transpose(fake_v_state), fake_h_prob) # Negative Divergence - h(v').v'^T\n",
    "\n",
    "            dW = pos_divergence-neg_divergence\n",
    "            dvb = v-fake_v_state\n",
    "            dhb = h_prob-fake_h_prob\n",
    "\n",
    "            # Similarity between reconstructed visible layer and input during training. \n",
    "            self.rec_error = tf.reduce_mean(tf.squared_difference(v, fake_v_state))\n",
    "            tf.summary.scalar('reconstruction_error', self.rec_error)\n",
    "\n",
    "            self.div = tf.reduce_mean(tf.abs(dW))\n",
    "            tf.summary.scalar('weights_increment', self.div)\n",
    "\n",
    "            return dW, dvb, dhb\n",
    "\n",
    "\n",
    "    def update(self, v, K=1):\n",
    "        batch_size = tf.cast(tf.shape(v)[0], tf.float32) # batch size\n",
    "        dW, dvb, dhb = self.CD(v, K=K) # contrastive divergence\n",
    "\n",
    "        delta_w = (self.lr/batch_size)*dW # weight gradient\n",
    "        delta_vb = (self.lr/batch_size)*(tf.reduce_sum(dvb, 0, keep_dims=True)) # visible bias gradient\n",
    "        delta_hb = (self.lr/batch_size)*(tf.reduce_sum(dhb, 0, keep_dims=True)) # hidden bias gradient\n",
    "\n",
    "        train_op = [self.W.assign_add(delta_w), self.vb.assign_add(delta_vb), self.hb.assign_add(delta_hb)] \n",
    "        return train_op\n",
    "\n",
    "\n",
    "    def gibbs(self, steps, v):\n",
    "        ''' Use the Gibbs sampler for a network of hidden and visible units. Return a sampled version of the input'''\n",
    "        with tf.name_scope(\"Gibbs_sampling\"):\n",
    "            for i in range(steps): # Number of steps to run the algorithm\n",
    "                hidden_p = self.get_probabilities('hidden', v) # v: input data\n",
    "                h = sample(hidden_p, mode=self.mode)\n",
    "\n",
    "                visible_p = self.get_probabilities('visible', h)\n",
    "                v = visible_p\n",
    "                #v = sample(visible_p)\n",
    "            return visible_p\n",
    "\n",
    "\n",
    "    def free_energy(self, v):\n",
    "        ''' Compute the free energy for a visible state'''\n",
    "        vbias_term = tf.matmul(v, tf.transpose(self.vb))\n",
    "        x_b = tf.matmul(v, self.W) + self.hb\n",
    "        hidden_term = tf.reduce_sum(tf.log(1 + tf.exp(x_b)))\n",
    "        return - hidden_term - vbias_term\n",
    "\n",
    "\n",
    "    def get_feature_map(self):\n",
    "        ''' Return hidden features'''\n",
    "        ft_map = {}\n",
    "        for k in range(self.n_hidden):\n",
    "            ft_map[k] = self.get_probabilities('visible', tf.expand_dims(tf.one_hot(k+1, self.n_hidden),0))\n",
    "        return ft_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate data\n",
    "X, y = make_classification(n_samples=1000, n_features=30, n_informative=20, n_classes=2,weights=[0.9,0.1])\n",
    "\n",
    "#train/test split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#minmax scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic samples using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect samples of the minority class\n",
    "pos_class_train=[]\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]==1:\n",
    "        pos_class_train.append(X_train[i])\n",
    "pos_class_train=np.array(pos_class_train)\n",
    "\n",
    "pos_class_test=[]\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i]==1:\n",
    "        pos_class_test.append(X_test[i])\n",
    "pos_class_test=np.array(pos_class_test)\n",
    "\n",
    "# generate synthetic samples\n",
    "synthetic_data=SMOTE(pos_class_train, 5, 15)\n",
    "synthetic_data=np.array(synthetic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply RBM to samples generated by SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mrzha\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction error at step 0: 0.149\n",
      "Reconstruction error at step 100: 0.027\n",
      "Reconstruction error at step 200: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 4494.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction error at step 300: 0.025\n",
      "Reconstruction error at step 400: 0.025\n",
      "Reconstruction error at step 500: 0.025\n",
      "Reconstruction error at step 600: 0.025\n",
      "Reconstruction error at step 700: 0.025\n",
      "Reconstruction error at step 800: 0.025\n",
      "Reconstruction error at step 900: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize RBM Model\n",
    "rbm_model = RBM(n_visible = 30, n_hidden = 5, lr = tf.constant(0.05, tf.float32), epochs = 1000, mode='continuous')\n",
    "\n",
    "# Placeholder for the visible layer of the RBM computation graph.\n",
    "v = tf.placeholder(tf.float32, shape=[None, rbm_model.n_visible], name=\"visible_layer\")\n",
    "\n",
    "# Update rule\n",
    "k=1\n",
    "train_op = rbm_model.update(v, K=k)\n",
    "\n",
    "# Free energy\n",
    "energy = rbm_model.free_energy(v=v)\n",
    "tf.summary.scalar('free_energy', tf.reduce_mean(energy))\n",
    "\n",
    "# Merge summaries for Tensorboard visualization\n",
    "summary = tf.summary.merge_all()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Training\n",
    "for epoch in tqdm(range(rbm_model.epochs)):\n",
    "    if epoch % 100 == 0:\n",
    "        result = sess.run([rbm_model.rec_error, summary], feed_dict = {v: pos_class_train.reshape(len(pos_class_train),-1).astype(np.float32)})\n",
    "        if epoch % 10 == 0: \n",
    "            print(\"Reconstruction error at step {}: {:.3f}\".format(epoch, result[0]))\n",
    "    sess.run(train_op, feed_dict = {v: pos_class_test.reshape(len(pos_class_test),-1).astype(np.float32)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1215)\n",
      "(100, 1215)\n",
      "(200, 1215)\n",
      "(300, 1215)\n",
      "(400, 1215)\n",
      "(500, 1215)\n",
      "(600, 1215)\n",
      "(700, 1215)\n",
      "(800, 1215)\n",
      "(900, 1215)\n",
      "(1000, 1215)\n",
      "(1100, 1215)\n",
      "(1200, 1215)\n"
     ]
    }
   ],
   "source": [
    "# Correct synthetic samples of SMOTE\n",
    "rbm_samples=[]\n",
    "for i in range(len(synthetic_data)):\n",
    "    if i % 100 == 0:\n",
    "        print((i,len(synthetic_data)))\n",
    "    rbm_syn=rbm_model.gibbs(1, v=v).eval(session=sess, feed_dict={v: synthetic_data[i].reshape(1,-1).astype(np.float32)})\n",
    "    rbm_samples.append(rbm_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification without synthetic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score of a logistic regression without synthetic samples is 0.3448275862068966\n"
     ]
    }
   ],
   "source": [
    "def to_label(prob,thres):\n",
    "    return (prob>=thres).astype(int)\n",
    "\n",
    "logit=LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "yhat=logit.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"f1_score of a logistic regression without synthetic samples is %s\" % f1_score(y_test,to_label(yhat, 0.5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with synthetic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_s=[item[0] for item in rbm_samples]\n",
    "rbm_s=np.array(rbm_s)\n",
    "X_syn=np.concatenate((X_train, rbm_s))\n",
    "y_syn=np.concatenate((y_train,np.array([1]*len(rbm_s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score of a logistic regression with synthetic samples is 0.6229508196721311\n"
     ]
    }
   ],
   "source": [
    "logit=LogisticRegression(random_state=0).fit(X_syn, y_syn)\n",
    "yhat=logit.predict_proba(X_test)[:,1]\n",
    "print(\"f1_score of a logistic regression with synthetic samples is %s\" % f1_score(y_test,to_label(yhat, 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
